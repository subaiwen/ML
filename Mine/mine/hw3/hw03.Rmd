---
title: \Large\textbf{Homework \#3}
fontsize: 11pt
author: \normalsize{Zhijian Liu}
geometry: margin=0.7in
output: 
  pdf_document: 
    highlight: haddock
    toc_depth: 1
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
invisible(lapply(c("knitr","kableExtra","dplyr","magrittr","ISLR"),library, character.only=TRUE))
```
\vspace{-0.5cm}
\textbf{\textcolor{red}{Multiple Linear Regression}}  
**1. Page 123, chap. 3, $\approx$ #10.**  Consider the following variables from the *Carseats* data set.  
```{r}
cbind(c("Sales", "Price", "Urban", "US"), c("Unit sales (in thousands) at each location", "Price company charges for car seats at each site", "A factor with levels No and Yes to indicate whether the store is in an urban or rural location","A factor with levels No and Yes to indicate whether the store is in the US or not")) %>% kable(booktabs = T) %>% column_spec(1, border_right = T) 
```

 (a) Fit a multiple regression model to predict Sales using Price, Urban, and US.
```{r 1.a, indent="     "}
attach(Carseats)
reg <- lm(Sales ~ Price + as.factor(Urban) + as.factor(US))
# arm::display(reg)
summary(reg)
```
 
 (b) Write out the model in equation form, being careful to handle the qualitative variables properly. That is, write a separate model for each category.  
 \newline
 $\begin{cases}
 Sales=13.04-0.05\cdot Price & \qquad \mbox{Urban = No, US = No}\\
 Sales=13.02-0.05\cdot Price & \qquad \mbox{Urban = Yes, US = No}\\
 Sales=14.24-0.05\cdot Price & \qquad \mbox{Urban = No, US = Yes}\\
 Sales=14.22-0.05\cdot Price & \qquad \mbox{Urban = Yes, US = Yes}
 \end{cases}$
 (c) For which of the predictors the null hypothesis $H_0 : \beta_j = 0$ is \underline{not} rejected? Verify your conclusion with the appropriate partial F-test (you can find examples in HW2 and the regression handout). Compare p-values of this test and the corresponding t-test. Also, compare the partial F-statistic with the squared t-statistic.  
 \newline
 From (a), we know that for Urban, the null hypothesis $H_0: \beta_j=0$ is not rejected. p-values of this test is the same as the corresponding t-test. The partial F-statistic is the same as the squared t-statistic as well.\newpage  
```{r 1.c, indent="     "}
full <- reg
reduce <- lm(Sales ~ Price + as.factor(US))
anova(full, reduce)
# compare p-values
out.1 <- cbind('t-test' = 0.936, 'F-test' = 0.9357)
row.names(out.1)  <- 'p-value'
out.1 %>%
  kable(escape = T, booktabs = T) %>%
  column_spec(2, border_left= F, border_right = F) %>%
  column_spec(1, border_left= F, border_right = T) 
# compare F-stat and t-stat ^2
out.2 <- cbind('$t^2$' = (-0.081)^2, 'F' = 0.0065) 
row.names(out.2)  <- 'statistics'
out.2 %>%
  kable(escape = F, booktabs = T) %>%
  column_spec(2, border_left= F, border_right = F) %>%
  column_spec(1, border_left= F, border_right = T) 
```
 
 
 (d) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.  
```{r 1.d, indent="     "}
arm::display(reduce)
```
 
 (e) Test linearity of Sales as a function of Price, according to our regression model. Is there a significant lack of fit?  
```{r 1.e,  indent="     "}
reg <- lm(Sales ~ Price)
lf <- lm(Sales ~ as.factor(Price))
anova(reg,lf)
```
 The lack of fit is not significant, so the linearity of Sales over Price is not violated.
 
 
\textbf{\textcolor{red}{Logistic Regression}}  
**2. Page 170, chap. 4, #9.**  This problem has to do with odds.  

 (a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?
 \begin{align*}
 \frac{\pi}{1-\pi} &= 0.37\\  
 \pi &= 0.270073
 \end{align*}
 The fraction of people is 0.270073 on average.  
 (b) Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?  
 \begin{align*}
 &\pi = 0.16\\
 &\frac{\pi}{1-\pi} = 0.1904762 
 \end{align*}
 The odds that she will default is 0.1904762.  
 
**3. Page 170, chap. 4, #6.**  Suppose we collect data for a group of students in a statistics class with variables $X_1$ =hours studied, $X_2$ =undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficients, $\hat{\beta_0}=-6$, $\hat{\beta_1} = 0.05$, $\hat{\beta_2} = 1$.

 (a) Estimate the probability that a student who studies for 40 hours and has an undergrad GPA of 3.5 gets an A in the class.  
 \begin{align*}
 log\frac{\hat{\pi}}{1-\hat{\pi}} &= \hat{\beta_0} + \hat{\beta_1}\cdot X_1 + \hat{\beta_2}\cdot X_2\\  
 log\frac{\hat{\pi}}{1-\hat{\pi}} &= -6 + 0.05 \cdot 40 + 1 \cdot 3.5\\
 log\frac{\hat{\pi}}{1-\hat{\pi}} &= -0.5\\
                        \hat{\pi} &= 0.3775407
 \end{align*}
 The probability that a student who studies for 40 hours and has an undergrad GPA of 3.5 gets an A in the class is 0.3775407.  
 (b) How many hours would the student in part (a) need to study to have a 50% (predicted) chance of getting an A in the class?  
 \begin{align*}
 log\frac{0.5}{1-0.5} &= -6 + 0.05 \cdot (X_1) + 1 \cdot 3.5\\
                    0 &= -2.5 + 0.05 \cdot (X_1)\\
                  X_1 &= 50
 \end{align*}
 The student need to study 50 hours to have a 50% chance of getting an A in the class.  
 
 
\textbf{\textcolor{red}{KNN}}  
**4. Pages 53-54, chap. 2, #7.** The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.  
```{r 4}
cbind('Obs' = c(1:6),
      '$X_1$' = c(0,2,0,0,-1,1),
      '$X_2$' = c(3,0,1,1,0,1),
      '$X_3$' = c(0,0,3,2,1,1),
      'Y' = c('Red','Red','Red','Green','Green','Red')) %>%
  kable(escape = F, booktabs = T) %>%
  kable_styling(position = "center") %>%
  column_spec(c(2,3,4), border_left= F, border_right = F) %>%
  column_spec(1, border_left= F, border_right = T) 
```
  Suppose we wish to use this data set to make a prediction for Y when $X_1 = X_2 = X_3 = 0$ using K-nearest neighbors.
  
  (a) Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$.
  \newline
```{r 4.a}
df <- data.frame('Obs' = c(1:6),
      'X1' = c(0,2,0,0,-1,1),
      'X2' = c(3,0,1,1,0,1),
      'X3' = c(0,0,3,2,1,1),
      'Y' = c('Red','Red','Red','Green','Green','Red'))
df %>% 
  mutate(distance = round(sqrt(X1**2+X2**2+X3**2),2)) %>%
  kable(escape = F, booktabs = T) %>%
  kable_styling(position = "center") %>%
  column_spec(c(2,3,4,5), border_left= F, border_right = F) %>%
  column_spec(1, border_left= F, border_right = T) 
```

  
  (b) What is our prediction with $K = 1$? Why?  
  Y would be Green, because the $5^{th}$ observation is the nearest neighbour with Y = Green.  
  
  (c) What is our prediction with K = 3? Why?  
  Y would be rend, because the $2^{th}$, $5^{th}$ and $6^{th}$ observations are the nearest neighbours with the mode of Y = Green.  
  
  (d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?
  \newline
  We expect a small K, since a small K stands for a flexible method that would produce a highly nonlinear boundary.  
  
**5.** When the number of features $p$ is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when $p$ is large. We will now investigate this curse.

 (a) Suppose that we have a set of observations, each with measurements on $p = 1$ feature, X. We assume that X is uniformly (evenly) distributed on [0, 1]. Associated with each observation is a response value. Suppose that we wish to predict a test observations response using only observations that are within 10% of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with $X = 0.6$, we will use observations in the range [0.55, 0.65]. On average, what fraction of the available observations will we use to make the prediction?
```{r 5.a, indent="     ", echo=T}
X <- runif(10000) 
predict.X <- X[which(X >= 0.55 & X <= 0.65)] # where x in [0.55, 0.65]
fraction <- length(predict.X)/length(X)
fraction
```
 $fraction = \frac{0.65-0.55}{1} = 0.1$. So 10% of the available observations will be used.  
 (b) Now suppose that we have a set of observations, each with measurements on $p = 2$ features, $X_1$ and $X_2$. We assume that ($X_1$ , $X_2$) are uniformly distributed on [0, 1] × [0, 1]. We wish to predict a test observations response using only observations that are within 10% of the range of $X_1$ and within 10% of the range of $X_2$ closest to that test observation. For instance, in order to predict the response for a test observation with $X_1$ = 0.6 and $X_2$ = 0.35, we will use observations in the range [0.55, 0.65] for $X_1$ and in the range [0.3, 0.4] for $X_2$. On average, what fraction of the available observations will we use to make the prediction?  
```{r 5.b, indent="     ", echo=T}
X1 <- X2 <- runif(10000) 
predict.X1 <- X1[which(X1 >= 0.55 & X1 <= 0.65)] # where x in [0.55, 0.65]
predict.X2 <- X2[which(X2 >= 0.3 & X2 <= 0.4)] # where x in [0.3, 0.4]
fraction <- (length(predict.X1) *length(predict.X2))/(length(X1)*length(X2))
fraction
```
 $fraction = \frac{(0.65-0.55)\times(0.4-0.3)}{1\times1} = 0.1$. So 1% of the available observations will be used.  
 (c) Now suppose that we have a set of observations on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observations response using observations within the 10% of each features range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?  
 \newline
 $fraction = \frac{(0.1)\times(0.1)\times(0.1)\times(0.1)\times\cdots \times(0.1)}{1\times1\times1\times1\times\cdots \times1} = 0.1^{100} = 10^{-100}$. So $10^{-100}$ of the available observations will be used.  
 (d) Using your answers to parts (a)(c), argue that a drawback of KNN when $p$ is large is that there are very few training observations “near” any given test observation.  
 \newline
 Using the same percentage of range within each feature, when p is large, the number of "near" training observations will be very small.  
 (e) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For p = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer.  
 \newline
 The length of each side of the hypercube would be 10% of the range of each feature, if each feature is uniformly distributed. In this example, the length would be 0.1.
 <!-- Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1, a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube. -->


