\documentclass[12pt,fleqn]{article}
\usepackage{pgfplots}
\pgfplotsset{compat=1.7}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}
\DeclareMathOperator{\rng}{Rng}
\DeclareMathOperator{\dom}{Dom}
\newcommand{\R}{\mathbb R}
\newcommand{\cont}{\subseteq}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage[mathscr]{euscript}
\let\euscr\mathscr \let\mathscr\relax% just so we can load this and rsfs
\usepackage[scr]{rsfso}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[colorlinks=true, pdfstartview=FitV, linkcolor=blue,
citecolor=blue, urlcolor=blue]{hyperref}

\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\newcommand{\ddx}{\frac{d}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\ddxp}[1]{\frac{d}{dx}\left( #1 \right)}
\newcommand{\dydx}{\frac{dy}{dx}}
\let\ds\displaystyle
\newcommand{\intx}[1]{\int #1 \, dx}
\newcommand{\intt}[1]{\int #1 \, dt}
\newcommand{\defint}[3]{\int_{#1}^{#2} #3 \, dx}
\newcommand{\imp}{\Rightarrow}
\newcommand{\un}{\cup}
\newcommand{\inter}{\cap}
\newcommand{\ps}{\mathscr{P}}
\newcommand{\set}[1]{\left\{ #1 \right\}}

\usepackage{enumerate} % enable \begin{enumerate}[1.]
\renewcommand{\labelenumi}{\alph{enumi}.} %first level: (a),(b)
\renewcommand{\labelenumii}{\roman{enumii}.} %second level: i,ii

\theoremstyle{definition}
\newtheorem*{sol}{Solution}
\newtheorem*{claim}{Claim}
\newtheorem{problem}{}
% ---------------------------------------------------------------------------------------------
\begin{document}
\lhead{Machine Learning}
\chead{Zhijian Liu}
\rhead{\today}

<<include=FALSE>>=
# Set global chunk options
opts_chunk$set(echo = T,
               warning = F,
               message = F,
               cache = T)
invisible(lapply(c("knitr","kableExtra","tidyverse","ISLR","bootstrap","MASS","boot"),library, character.only=TRUE))
@

% Just put your proofs in between the \begin{proof} and the \end{proof} statements!

\section*{Homework \#6}
% 1.
	\begin{problem} \textbf{(Jackknife and Bootstrap, continuing from the previous h/w)} Using your knowledge of the definition expected value complete the following:
		One needs to estimate $\theta$, the frequency of days with 0 traffic accidents on a certain highway. The data are collected. During 40 days, there are 26 days with 0 accidents, 10 days with 1 accident, and 4 days with 2 accidents.\\
  Statistician A estimates $\theta$ with a sample proportion $\hat{p} = 26/40 = 0.65$.\\
  Statistician B argues that this method does not distinguish between the days with 1 accident and the days with 2 accidents, losing some valuable information. She suggests to model the number of accidents X by a Poisson distribution with parameter $\lambda$. Then we have $\theta = \boldmath{P}\{X = 0\} = exp(-\lambda)$. She estimates $\lambda$ with $\hat{\lambda} = \bar{X}$. Then $\hat{\theta} = exp(-\hat{\lambda})$. However, this estimator is biased.
    \begin{enumerate}[(a)]
    % (a)
      \item Now we have three competing estimators - $\hat{p}$, $\hat{\theta}$, and $\hat{\theta_{JK}}$. Use bootstrap to estimate their standard deviations.
<<1.a>>=
set.seed(666)
accident <- sample( c(rep(0,26),rep(1,10),rep(2,4)) )
B <- 10000
n <- length(accident)
# estimations
p <- function(x){return( mean(x == 0) )} # p.hat
theta <- function(x){return( exp( - mean(x) ) )} # theta.hat
theta.jk <- function(x){ # theta.hat.jk
  jk <- theta(x) - jackknife( x, theta )$jack.bias
  return(jk)}
# container
p.boot <- theta.boot <- theta.jk.boot <- rep(NA,n)
# std. p.hat
for (i in 1:B){
  clone <- sample(accident, n, replace = T)
  p.boot[i] <- p(clone)
  theta.boot[i] <- theta(clone)
  theta.jk.boot[i] <- theta.jk(clone)
}
kable(cbind('$\\hat{Std}(\\hat{p})$' = sd(p.boot),
            '$\\hat{Std}(\\hat{\\theta})$' = sd(theta.boot),
            '$\\hat{Std}(\\hat{\\theta_{JK}})$' = sd(theta.jk.boot)),
      escape = F)
@

    % (b)
      \item Compare our three estimators of $\theta$ according to their bias and standard error.\\
      $\hat{p}$ is an unbias estimator, but since it lose some valuable information, it has the highest standard error among three estimators. $\hat{\theta_{JK}}$ slightly reduces the bias of the $\hat{\theta}$, but also increases the standard error at the meanwhile.
		\end{enumerate}
	\end{problem}
% 2.
  \begin{problem} We will now consider the \texttt{Boston} housing data set, from the \texttt{MASS} library.
    \begin{enumerate}[(a)]
    % a.
      \item Based on this data set, provide an estimate for the population mean $\mu$ of \texttt{medv}, which is the median value of owner-occupied homes in \$1000s. Call this estimate $\hat{\mu}$.
<<2.a>>=
mu.hat <- mean(Boston$medv)
mu.hat
@
      The estimation: $\hat{\mu} = \sum_{i=1}^{n}medv_i = 22.53281$
    % b.
      \item Provide an estimate of the standard error of $\hat{\mu}$ (as we know, $Std\bar{X} = \sigma/\sqrt{n}$).
<<2.b>>=
n <- nrow(Boston)
s <- sd(Boston$medv)/sqrt(n)
s
@
      An estimate of the standard error of $\hat{\mu}$:  $Std(\bar{X}) = \sqrt{\frac{\sum_{i=1}^{n}(X_i - \bar{X})^2}{n-1}} = 0.4088611$
    % c.
      \item Now estimate the standard error of $\hat{\mu}$ using the \texttt{bootstrap}. How does this compare to your answer from (b)?
<<2.c>>=
set.seed(666)
mu <- function(x,sample) {return( mean(x[sample]) )}
boot(Boston$medv, mu, R = 10000)
@
      The estimated standard error of $\hat{\mu}$ under bootstrap: $\hat{Std}(\bar{X}_{boot}) = 0.4058625$. It is very closed to the estimation in part (b).
    % d.
      \item Based on your bootstrap estimate from (c), provide a 95 \% confidence interval for $\mu$. A popular approximation is $\hat{\mu} \pm 2 \hat{Std}(\hat{\mu})$. Compare it to the results obtained using \texttt{R} command \texttt{t.test(Boston\$medv)}.
<<2.d>>=
# bootstrap result
mu.boot <- boot(Boston$medv, mu, R = 10000)$t
cbind('lower bound' = mean(mu.boot)-2*sd(mu.boot),
      'upper bound' = mean(mu.boot)+2*sd(mu.boot))
# t.test
t.test(Boston$medv)
@
      The 95\% confidence interval for $\mu$ under bootstrap method is (21.71768, 23.35014), and that under t-test is (21.72953, 23.33608). Two intervals are very similar.
    % e.
      \item Now, estimate M, the population median of \texttt{medv} with the sample median $\hat{M}$.
<<2.e>>=
m.hat <- median(Boston$medv)
m.hat
@
      The sample median: $\hat{M}$ = 21.2
    % f.
      \item We now would like to estimate the standard error of $\hat{M}$, but unfortunately, there is no simple formula for computing the standard error of a sample median. Instead, estimate this standard error using the bootstrap.
<<2.f>>=
m <- function(x,sample){return( median(x[sample]) )}
boot(Boston$medv, m, R = 10000)
@
      The estimated standard error $\hat{Std}(\hat{M}_{boot}) = 0.3761129$

    \end{enumerate}
  \end{problem}
\end{document}
